{
    "DateTime": "2023-12-27 14:40:11",
    "URL": "https://support.timextender.com/odx-107/benefits-of-odx-instances-1569",
    "Keywords": "missing",
    "Title": "Benefits of ODX instances _ Community",
    "Author": "Christian Hauggaard",
    "Text": "This article describes the benefits of ODX instances. Please see some of thekey benefits below: @Christian Hauggaard @daniel I cannot provide an update for Delta Lake at this time, although I know it is being investigated. Regarding the ADF limit, this still seems to be the case, please see the below article for more info: https://learn.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features  @Christian Hauggaard @Christian Hauggaard  https://learn.microsoft.com/en-us/azure/data-factory/connector-sql-server?tabs=data-factory#parallel-copy-from-sql-database  \"- Copy from partition-option-enabled data stores (including Azure Database for PostgreSQL, Azure SQL Database, Azure SQL Managed Instance, Azure Synapse Analytics, Oracle, Netezza, SQL Server, and Teradata): 2-256 when writing to a folder, and 2-4 when writing to one single file. Note per source data partition can use up to 4 DIUs. if the ODX Server is loading from one of these DB sources and large source tables are partitioned, you can reach more DIU for that table transfer. Individual chunks will never go over 4 DIU. With the current folder structure in the ODX ADLS, I think only large tables could be partitioned to reach a higher speed. Delta-parquet will already mitigate day-to-day performance, but initial load would still benefit. If you want to speed up ADF, you should run self-hosted integration runtimes (you can scale up the VM as far as your budget will allow). These can be clustered to allow for more performance. As you usually load data from other networks, I typically use a self-hosted integration runtime for source-->ODX transfers and AzureRuntimes for ODX to DWH. If the ODX-->DWH transfer becomes a bottleneck, that can be migrated to a separate self-hosted integration runtime. @rory.smith Hi @daniel,  you can also run the self-hosted integration runtime on your ODX Server /TX machine if you want, as long as you have enough RAM free. Note that for ADF the packing/unpacking of parquet is done in the memory space of your runtime, be it Azure or self-hosted. Using ADO.net it is done on your ODX Server VM;this is the reason for the limit memory use setting.  The parquet files aren't really optimized for IOPS speed (column cardinality and sorting would improve this), so the parquet packing/unpacking probably adds a relatively high overhead due to file bloat. ADF also generally seems slow because an AzureRuntime can take 60 seconds to spin up (you can tweak settings to keep them alive). ",
    "Lists": []
}